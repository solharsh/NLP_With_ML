{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors Model for a Regression Problem\n",
    "\n",
    "In this notebook, we use the K Nearest Neighbors method to build a regressor to predict the __log_votes__ field of our review dataset.\n",
    "\n",
    "\n",
    "1. Reading the dataset\n",
    "2. Exploratory data analysis and missing value imputation\n",
    "3. Stop word removal and stemming\n",
    "4. Scaling numerical fields\n",
    "5. Splitting the training dataset into training and validation\n",
    "6. Computing Bag of Words features\n",
    "7. Fitting the regression model\n",
    "8. Checking model performance on the validation dataset\n",
    "9. Trying different K values\n",
    "10. Checking few other things to see if performance can be improved\n",
    "\n",
    "Overall dataset schema:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __rating:__ Rating of the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes). *This field is a processed version of the votes field. People can click on the \"helpful\" button when they find a customer review helpful. This increases the vote by 1. __log_votes__ is calculated like this log(1+votes). This formulation helps us get a smaller range for votes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading the dataset\n",
    "\n",
    "We will use the __pandas__ library to read our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/aws-samples/aws-machine-learning-embark-nlp/master/DATA/NLP/EMBK-NLP-REVIEW-DATA-CSV.csv'\n",
    "#df = pd.read_csv(url,index_col=0,parse_dates=[0])\n",
    "df = pd.read_csv(url,parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>log_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Stuck with this at work, slow and we still got...</td>\n",
       "      <td>Use SEP or Mcafee</td>\n",
       "      <td>False</td>\n",
       "      <td>1464739200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I use parallels every day with both my persona...</td>\n",
       "      <td>Use it daily</td>\n",
       "      <td>False</td>\n",
       "      <td>1332892800</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Barbara Robbins\\n\\nI've used TurboTax to do ou...</td>\n",
       "      <td>Helpful Product</td>\n",
       "      <td>True</td>\n",
       "      <td>1398816000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>I have been using this software security for y...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1430784000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>If you want your computer hijacked and slowed ...</td>\n",
       "      <td>... hijacked and slowed to a crawl Windows 10 ...</td>\n",
       "      <td>False</td>\n",
       "      <td>1508025600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  Stuck with this at work, slow and we still got...   \n",
       "1  I use parallels every day with both my persona...   \n",
       "2  Barbara Robbins\\n\\nI've used TurboTax to do ou...   \n",
       "3  I have been using this software security for y...   \n",
       "4  If you want your computer hijacked and slowed ...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                                  Use SEP or Mcafee     False  1464739200   \n",
       "1                                       Use it daily     False  1332892800   \n",
       "2                                    Helpful Product      True  1398816000   \n",
       "3                                         Five Stars      True  1430784000   \n",
       "4  ... hijacked and slowed to a crawl Windows 10 ...     False  1508025600   \n",
       "\n",
       "   rating  log_votes  \n",
       "0     1.0        0.0  \n",
       "1     5.0        0.0  \n",
       "2     4.0        0.0  \n",
       "3     5.0        0.0  \n",
       "4     1.0        0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is: (55000, 6)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reviewText', 'summary', 'verified', 'time', 'rating', 'log_votes'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory data analysis and missing value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"log_votes\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.799753318287247"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"log_votes\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"log_votes\"].plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of missing values for each columm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewText    6\n",
      "summary       7\n",
      "verified      0\n",
      "time          0\n",
      "rating        0\n",
      "log_votes     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill-in the missing values for __reviewText__ below. We will just use the placeholder \"Missing\" here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reviewText\"].fillna(\"Missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"summary\"].fillna(\"Missing\", inplace=True) #similarly, placeholders for summary column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stop word removal and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\solharsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\solharsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the library and functions\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the stop word removal and text cleaning processes below. NLTK library provides a list of common stop words. We will use the list, but remove some of the words from that list. It is because those words are actually useful to understand the sentiment in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Let's get a list of stop words from the NLTK library\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# These words are important for our problem. We don't want to remove them.\n",
    "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \n",
    "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \n",
    "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "# New stop word list\n",
    "stop_words = [word for word in stop if word not in excluding]\n",
    "\n",
    "snow = SnowballStemmer('english')\n",
    "\n",
    "def process_text(texts): \n",
    "    final_text_list=[]\n",
    "    for sent in texts:\n",
    "        filtered_sentence=[]\n",
    "        \n",
    "        sent = sent.lower() # Lowercase \n",
    "        sent = sent.strip() # Remove leading/trailing whitespace\n",
    "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n",
    "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n",
    "        \n",
    "        for w in word_tokenize(sent):\n",
    "            # We are applying some custom filtering here.\n",
    "            # Check if it is not numeric and its length>2 and not in stop words\n",
    "            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words):  \n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "        final_string = \" \".join(filtered_sentence) #final string of cleaned words\n",
    " \n",
    "        final_text_list.append(final_string)\n",
    "    \n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing the reviewText field\n"
     ]
    }
   ],
   "source": [
    "print(\"Pre-processing the reviewText field\")\n",
    "df[\"reviewText\"] = process_text(df[\"reviewText\"].tolist()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scaling numerical fields:\n",
    "\n",
    "We will apply min-max scaling to our rating field so that they will be between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rating\"] = (df[\"rating\"] - df[\"rating\"].min())/(df[\"rating\"].max()-df[\"rating\"].min())\n",
    "df[\"time\"] = (df[\"time\"] - df[\"time\"].min())/(df[\"time\"].max()-df[\"time\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Splitting the training dataset into training and validation\n",
    "\n",
    "Sklearn library has a useful function to split datasets. We will use the __train_test_split()__ function. In the example below, we get 90% of the data for training and 10% is left for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Input: \"reviewText\", \"rating\" and \"time\"\n",
    "# Target: \"log_votes\"\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[[\"reviewText\", \"rating\", \"time\"]],\n",
    "                                                  df[\"log_votes\"].tolist(),\n",
    "                                                  test_size=0.10,\n",
    "                                                  shuffle=True\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Computing Bag of Words features\n",
    "\n",
    "We are using binary features here. TF and TF-IDF are also other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Initialize the binary count vectorizer\n",
    "tfidf_vectorizer = CountVectorizer(binary=True,\n",
    "                                   max_features=50    # Limit the vocabulary size\n",
    "                                  )\n",
    "# Fit and transform\n",
    "X_train_text_vectors = tfidf_vectorizer.fit_transform(X_train[\"reviewText\"].tolist())\n",
    "# Only transform\n",
    "X_val_text_vectors = tfidf_vectorizer.transform(X_val[\"reviewText\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print our vocabulary below. The number next to the word is its index in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'work': 47, 'product': 27, 'use': 39, 'year': 49, 'version': 42, 'problem': 26, 'download': 6, 'instal': 15, 'time': 36, 'new': 22, 'upgrad': 38, 'good': 12, 'softwar': 33, 'price': 25, 'need': 21, 'recommend': 31, 'would': 48, 'buy': 3, 'window': 46, 'not': 23, 'get': 11, 'great': 13, 've': 41, 'look': 17, 'make': 18, 'want': 43, 'support': 35, 'help': 14, 'way': 44, 'still': 34, 'tri': 37, 'file': 9, 'back': 1, 'find': 10, 'better': 2, 'also': 0, 'easi': 7, 'much': 20, 'run': 32, 'well': 45, 'could': 5, 'comput': 4, 'mani': 19, 'realli': 30, 'like': 16, 'program': 28, 'even': 8, 'user': 40, 'one': 24, 'purchas': 29}\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Fitting the regression model\n",
    "\n",
    "We will use __KNeighborsRegressor__ from Sklearn library with __n_neighbors__ = 5. We will try different values in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                    weights='uniform')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "\n",
    "# Let' merge our features\n",
    "X_train_features = np.column_stack((X_train_text_vectors.toarray(), \n",
    "                                    X_train[\"rating\"].values, \n",
    "                                    X_train[\"time\"].values)\n",
    "                                  )\n",
    "\n",
    "# Using the default KNN with 5 nearest neighbors\n",
    "knnRegressor = KNeighborsRegressor(n_neighbors=5)\n",
    "knnRegressor.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Checking model performance on the validation dataset\n",
    "\n",
    "We kept some of our data as validation data. Let's check model performance on this validation dataset. \n",
    "\n",
    "One evaluation metrics for regression problems is the Mean Squared Error ($\\mathrm{MSE}$), defined as:\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2,\n",
    "$$\n",
    "measuring the mean of all squared differences between the data values $y_i$ and the predicted values $\\hat{y_i}$, where $n$ is the number of data records.\n",
    "\n",
    "Another regression evaluation metric is $\\mathrm{R^2}$, measuring the fraction of the variance in the dataset our model can explain:\n",
    "$$\n",
    "\\mathrm{R^2} = 1- \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2},\n",
    "$$\n",
    "where $\\bar y = \\frac{1}{n}\\sum_{i = 1}^n y_i$ is the mean value of the data values $y_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Training and validation time for one value of K (in seconds): 45.17020630836487\n",
      "Mean_squared_error: 0.805364,  R_square_score: 0.117523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "X_val_features = np.column_stack((X_val_text_vectors.toarray(), \n",
    "                                  X_val[\"rating\"].values, \n",
    "                                  X_val[\"time\"].values))\n",
    "\n",
    "val_predictions = knnRegressor.predict(X_val_features)\n",
    "\n",
    "end = time()\n",
    "print('KNN Training and validation time for one value of K (in seconds):', end-start)\n",
    "\n",
    "print(\"Mean_squared_error: %f,  R_square_score: %f\" % (mean_squared_error(y_val, val_predictions), r2_score(y_val, val_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Trying different K values\n",
    "\n",
    "Let's try different K values and see how the model performs with each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=5, Mean_squared_error: 0.805364,  R_square_score: 0.117523\n",
      "K=10, Mean_squared_error: 0.770161,  R_square_score: 0.156096\n",
      "K=20, Mean_squared_error: 0.759969,  R_square_score: 0.167264\n",
      "K=25, Mean_squared_error: 0.757802,  R_square_score: 0.169639\n",
      "K=30, Mean_squared_error: 0.757024,  R_square_score: 0.170491\n",
      "K=40, Mean_squared_error: 0.755396,  R_square_score: 0.172275\n",
      "K=50, Mean_squared_error: 0.756126,  R_square_score: 0.171475\n"
     ]
    }
   ],
   "source": [
    "K_values = [5, 10, 20, 25, 30, 40, 50]\n",
    "\n",
    "for K in K_values:\n",
    "    knnRegressor = KNeighborsRegressor(n_neighbors=K)\n",
    "    knnRegressor.fit(X_train_features, y_train)\n",
    "    val_predictions = knnRegressor.predict(X_val_features)\n",
    "    print(\"K=%d, Mean_squared_error: %f,  R_square_score: %f\" % (K, mean_squared_error(y_val, val_predictions), r2_score(y_val, val_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts\n",
    "\n",
    "Word counts can be simply calculated using the same __CountVectorizer()__ function __without__ the __binary__ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Initialize the binary count vectorizer\n",
    "tfidf_vectorizer = CountVectorizer(max_features=50)    # Limit the vocabulary size\n",
    "# Fit and transform\n",
    "X_train_text_vectors = tfidf_vectorizer.fit_transform(X_train[\"reviewText\"].tolist())\n",
    "# Only transform\n",
    "X_val_text_vectors = tfidf_vectorizer.transform(X_val[\"reviewText\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the regression model with Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                    weights='uniform')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "\n",
    "# Let' merge our features\n",
    "X_train_features = np.column_stack((X_train_text_vectors.toarray(), \n",
    "                                    X_train[\"rating\"].values, \n",
    "                                    X_train[\"time\"].values)\n",
    "                                  )\n",
    "\n",
    "# Using the default KNN with 5 nearest neighbors\n",
    "knnRegressor = KNeighborsRegressor(n_neighbors=5)\n",
    "knnRegressor.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model performance with Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Training and validation time for one value of K (in seconds): 42.620521783828735\n",
      "Mean_squared_error: 0.822351,  R_square_score: 0.098909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "X_val_features = np.column_stack((X_val_text_vectors.toarray(), \n",
    "                                  X_val[\"rating\"].values, \n",
    "                                  X_val[\"time\"].values))\n",
    "\n",
    "val_predictions = knnRegressor.predict(X_val_features)\n",
    "\n",
    "end = time()\n",
    "print('KNN Training and validation time for one value of K (in seconds):', end-start)\n",
    "\n",
    "print(\"Mean_squared_error: %f,  R_square_score: %f\" % (mean_squared_error(y_val, val_predictions), r2_score(y_val, val_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE value has been increased and time has been cut down to approximately half. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the regression model with TF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(use_idf=False,max_features=50)\n",
    "\n",
    "# Fit and transform\n",
    "X_train_text_vectors = tf_vectorizer.fit_transform(X_train[\"reviewText\"].tolist())\n",
    "# Only transform\n",
    "X_val_text_vectors = tf_vectorizer.transform(X_val[\"reviewText\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                    weights='uniform')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "\n",
    "# Let' merge our features\n",
    "X_train_features = np.column_stack((X_train_text_vectors.toarray(), \n",
    "                                    X_train[\"rating\"].values, \n",
    "                                    X_train[\"time\"].values)\n",
    "                                  )\n",
    "\n",
    "# Using the default KNN with 5 nearest neighbors\n",
    "knnRegressor = KNeighborsRegressor(n_neighbors=5)\n",
    "knnRegressor.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model performance with TF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Training and validation time for one value of K (in seconds): 44.515535831451416\n",
      "Mean_squared_error: 0.747066,  R_square_score: 0.181403\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "X_val_features = np.column_stack((X_val_text_vectors.toarray(), \n",
    "                                  X_val[\"rating\"].values, \n",
    "                                  X_val[\"time\"].values))\n",
    "\n",
    "val_predictions = knnRegressor.predict(X_val_features)\n",
    "\n",
    "end = time()\n",
    "print('KNN Training and validation time for one value of K (in seconds):', end-start)\n",
    "\n",
    "print(\"Mean_squared_error: %f,  R_square_score: %f\" % (mean_squared_error(y_val, val_predictions), r2_score(y_val, val_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the regression model with TF IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True,max_features=50)\n",
    "\n",
    "# Fit and transform\n",
    "X_train_text_vectors = tfidf_vectorizer.fit_transform(X_train[\"reviewText\"].tolist())\n",
    "# Only transform\n",
    "X_val_text_vectors = tfidf_vectorizer.transform(X_val[\"reviewText\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                    weights='uniform')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "\n",
    "# Let' merge our features\n",
    "X_train_features = np.column_stack((X_train_text_vectors.toarray(), \n",
    "                                    X_train[\"rating\"].values, \n",
    "                                    X_train[\"time\"].values)\n",
    "                                  )\n",
    "\n",
    "# Using the default KNN with 5 nearest neighbors\n",
    "knnRegressor = KNeighborsRegressor(n_neighbors=5)\n",
    "knnRegressor.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model performance with TF IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Training and validation time for one value of K (in seconds): 48.21263289451599\n",
      "Mean_squared_error: 0.744468,  R_square_score: 0.184250\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "X_val_features = np.column_stack((X_val_text_vectors.toarray(), \n",
    "                                  X_val[\"rating\"].values, \n",
    "                                  X_val[\"time\"].values))\n",
    "\n",
    "val_predictions = knnRegressor.predict(X_val_features)\n",
    "\n",
    "end = time()\n",
    "print('KNN Training and validation time for one value of K (in seconds):', end-start)\n",
    "\n",
    "print(\"Mean_squared_error: %f,  R_square_score: %f\" % (mean_squared_error(y_val, val_predictions), r2_score(y_val, val_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Linear Regression models and checking the validation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first fit LinearRegression from Sklearn library, and check the performance on the validation dataset. Using the coef_ atribute, we can also print the learned weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression on validation: Mean_squared_error: 0.633632, R_squared_score: 0.305698\n",
      "LinearRegression model weights: \n",
      " [ 0.37882511  0.18569065  0.17342977  0.02810875  0.0639604   0.14227191\n",
      "  0.06879892  0.26993592  0.41287513  0.33034913  0.18254976  0.12033163\n",
      "  0.13407989  0.19383979  0.17489861  0.17098964  0.34341139  0.40548892\n",
      "  0.17452126  0.1435772   0.28559151  0.1584397   0.25739598  0.05910168\n",
      "  0.147466    0.33784033  0.20604816  0.13204411  0.18432771  0.17911992\n",
      "  0.32560781  0.42584004  0.21823761 -0.09139284  0.29086186  0.14029278\n",
      "  0.24292381  0.14699231  0.28389383  0.06479309  0.31988433  0.33665088\n",
      "  0.27666987  0.37050821  0.35051092  0.14869107  0.31643034  0.08851016\n",
      "  0.08274091  0.16984067 -0.36073012 -1.77327376]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "lrRegressor = LinearRegression()\n",
    "lrRegressor.fit(X_train_features,y_train)\n",
    "lrRegressor_val_predictions = lrRegressor.predict(X_val_features)\n",
    "print(\"LinearRegression on validation: Mean_squared_error: %f, R_squared_score: %f\" %\\\n",
    "      (mean_squared_error(y_val, lrRegressor_val_predictions), r2_score(y_val, lrRegressor_val_predictions)))\n",
    "print(\"LinearRegression model weights: \\n\", lrRegressor.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge (Linear Regression with L2 regularization)\n",
    "\n",
    "Let's now fit Ridge from Sklearn library, and check the performance on the validation dataset.\n",
    "\n",
    "To improve the performance of a LinearRegression model, Ridge is tuning model complexity by adding a L2\n",
    "\n",
    "penalty score for complexity to the model cost function:\n",
    "\n",
    "Cregularized(w)=C(w)+alpha∗||w||2\n",
    "\n",
    "where w\n",
    "is the model weights vector, and ||w||2=∑w2i\n",
    "\n",
    ".The strength of the regularization is controlled by the regularizer parameter, alpha: smaller value of alpha\n",
    ", weaker regularization; larger value of alpha, stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge on Validation: Mean_squared_error: 0.633083,  R_square_score: 0.306300\n",
      "Ridge model weights: \n",
      " [ 0.33710393  0.16658409  0.15460978  0.02826003  0.06426838  0.11672356\n",
      "  0.05168662  0.24772996  0.36537312  0.29546611  0.18055831  0.09548388\n",
      "  0.10648171  0.17595235  0.16534583  0.16655178  0.31450348  0.35251353\n",
      "  0.15983564  0.13342621  0.26831181  0.15269216  0.25003895  0.06598874\n",
      "  0.13691095  0.32688851  0.18119864  0.12074688  0.16949043  0.16964397\n",
      "  0.31431003  0.3724076   0.20031923 -0.0952992   0.25637325  0.14110337\n",
      "  0.22604656  0.13542405  0.25594905  0.06901324  0.28542752  0.30045278\n",
      "  0.26867991  0.33089162  0.30612365  0.13716068  0.29009699  0.06944907\n",
      "  0.08760068  0.15398252 -0.3657378  -1.71269673]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "ridgeRegressor = Ridge(alpha = 100)\n",
    "ridgeRegressor.fit(X_train_features, y_train)\n",
    "ridgeRegressor_val_predictions = ridgeRegressor.predict(X_val_features)\n",
    "\n",
    "print(\"Ridge on Validation: Mean_squared_error: %f,  R_square_score: %f\" % \\\n",
    "    (mean_squared_error(y_val, ridgeRegressor_val_predictions),r2_score(y_val, ridgeRegressor_val_predictions)))\n",
    "      \n",
    "print(\"Ridge model weights: \\n\", ridgeRegressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=50, Ridge on Validation: Mean_squared_error: 0.633155,  R_square_score: 0.306221\n",
      "K=100, Ridge on Validation: Mean_squared_error: 0.633083,  R_square_score: 0.306300\n",
      "K=150, Ridge on Validation: Mean_squared_error: 0.633323,  R_square_score: 0.306037\n",
      "K=200, Ridge on Validation: Mean_squared_error: 0.633808,  R_square_score: 0.305505\n",
      "K=250, Ridge on Validation: Mean_squared_error: 0.634492,  R_square_score: 0.304756\n"
     ]
    }
   ],
   "source": [
    "alpha_values = [50, 100, 150, 200, 250]\n",
    "\n",
    "for A in alpha_values:\n",
    "    ridgeRegressor = Ridge(alpha = A)\n",
    "    ridgeRegressor.fit(X_train_features, y_train)\n",
    "    ridgeRegressor_val_predictions = ridgeRegressor.predict(X_val_features)\n",
    "    print(\"K=%d, Ridge on Validation: Mean_squared_error: %f,  R_square_score: %f\" % \\\n",
    "    (A, mean_squared_error(y_val, ridgeRegressor_val_predictions),r2_score(y_val, ridgeRegressor_val_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO (Linear Regression with L1 regularization)\n",
    "\n",
    "Let's also fit Lasso from Sklearn library, and check the performance on the validation dataset.\n",
    "\n",
    "Lasso is tuning model complexity by adding a L1\n",
    "\n",
    "penalty score for complexity to the model cost function:\n",
    "\n",
    "Cregularized(w)=C(w)+alpha∗||w||1\n",
    "\n",
    "where w\n",
    "is the model weights vector, and ||w||1=∑|wi|\n",
    "\n",
    "Again, the strength of the regularization is controlled by the regularizer parameter, alpha\n",
    "\n",
    "Due to the geometry of L1 norm, with Lasso, some of the weights will shrink all the way to 0, leading to sparsity - some of the features are not contributing to the model afterall!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso on validation: Mean_Squared_Error: 0.633926, R_Squared_Score: 0.305375\n",
      "Lasso model weights: \n",
      " [ 0.31381443  0.0938786   0.0809034   0.          0.          0.06812602\n",
      "  0.          0.19630092  0.34483799  0.25658037  0.14749587  0.02668533\n",
      "  0.04296748  0.11003029  0.12961076  0.12270943  0.28102288  0.32527102\n",
      "  0.09078496  0.08031686  0.23764603  0.11713189  0.21727912  0.\n",
      "  0.08955     0.29907129  0.11975441  0.05752566  0.10291502  0.10257788\n",
      "  0.28840594  0.35393882  0.12363913 -0.08514955  0.20377918  0.09995988\n",
      "  0.18997859  0.07227622  0.20873718  0.03111594  0.23784724  0.27704558\n",
      "  0.24775554  0.30290928  0.26299041  0.07725164  0.27013192  0.02275971\n",
      "  0.03600531  0.1085075  -0.37706349 -1.85914313]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "LassoRegressor = Lasso(alpha=0.001)\n",
    "LassoRegressor.fit(X_train_features, y_train)\n",
    "LassoRegressor_val_predictions = LassoRegressor.predict(X_val_features)\n",
    "print(\"Lasso on validation: Mean_Squared_Error: %f, R_Squared_Score: %f\" % \\\n",
    "    (mean_squared_error(y_val, LassoRegressor_val_predictions), r2_score(y_val, LassoRegressor_val_predictions)))\n",
    "      \n",
    "print(\"Lasso model weights: \\n\", LassoRegressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_values=0.001000, Ridge on Validation: Mean_squared_error: 0.633926,  R_square_score: 0.305375\n",
      "alpha_values=0.005000, Ridge on Validation: Mean_squared_error: 0.657326,  R_square_score: 0.279735\n",
      "alpha_values=0.010000, Ridge on Validation: Mean_squared_error: 0.669026,  R_square_score: 0.266915\n",
      "alpha_values=0.050000, Ridge on Validation: Mean_squared_error: 0.744561,  R_square_score: 0.184147\n",
      "alpha_values=0.010000, Ridge on Validation: Mean_squared_error: 0.669026,  R_square_score: 0.266915\n"
     ]
    }
   ],
   "source": [
    "alpha_values_Lasso = [0.001, 0.005, 0.010, 0.050, 0.01]\n",
    "\n",
    "for A in alpha_values_Lasso:\n",
    "    LassoRegressor = Lasso(alpha = A)\n",
    "    LassoRegressor.fit(X_train_features, y_train)\n",
    "    LassoRegressor_val_predictions = LassoRegressor.predict(X_val_features)\n",
    "    print(\"alpha_values=%f, Ridge on Validation: Mean_squared_error: %f,  R_square_score: %f\" % \\\n",
    "    (A, mean_squared_error(y_val, LassoRegressor_val_predictions),r2_score(y_val, LassoRegressor_val_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mean Squared Error significantly improved with alpha value of 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticNet (Linear Regression with L2 and L1 regularization)\n",
    "\n",
    "Let's finally try ElasticNet from Sklearn library, and check the performance on the validation dataset.\n",
    "\n",
    "ElasticNet is tuning model complexity by adding both L2 and L1 penalty scores for complexity to the model's cost function:\n",
    "\n",
    "Cregularized(w)=C(w)+0.5∗alpha∗(1−l1ratio)||w||22+alpha∗l1ratio∗||w||1\n",
    "\n",
    "and using two parameters, alpha and l1 ratio, to control the strength of the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net on validation: Mean_Squared Error: 0.633041, R_Squared_Error: 0.306345\n",
      "ElasticNet model weights: \n",
      " [ 0.35249824  0.16806482  0.15566804  0.02289057  0.05673637  0.12303015\n",
      "  0.05210809  0.25253976  0.3834154   0.30677032  0.17844723  0.0994309\n",
      "  0.11223518  0.177499    0.16649298  0.16442222  0.32395121  0.37212187\n",
      "  0.15964692  0.13277887  0.2731316   0.15174482  0.25037761  0.05672899\n",
      "  0.13690768  0.32919836  0.18596994  0.11959284  0.16967463  0.16780457\n",
      "  0.31690599  0.39304282  0.20102115 -0.09283223  0.26593785  0.13699201\n",
      "  0.23003903  0.1345208   0.26338172  0.06390058  0.29562912  0.31349956\n",
      "  0.27021032  0.34516084  0.32083155  0.13651615  0.29952775  0.07307907\n",
      "  0.08077847  0.15645405 -0.364623   -1.75429137]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "enRegressor = ElasticNet(alpha = 0.001, l1_ratio=0.1)\n",
    "enRegressor.fit(X_train_features,y_train)\n",
    "enRegressor_val_predictions = enRegressor.predict(X_val_features)\n",
    "print(\"Elastic Net on validation: Mean_Squared Error: %f, R_Squared_Error: %f\" % \\\n",
    "     (mean_squared_error(y_val, enRegressor_val_predictions),r2_score(y_val,enRegressor_val_predictions)))\n",
    "print(\"ElasticNet model weights: \\n\", enRegressor.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights shrinkage and sparsity\n",
    "\n",
    "Let's compare weights ranges for all these regression models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression weights range: \n",
      " 0.02810874782481021 1.7732737578052393\n",
      "Ridge weights range: \n",
      " 0.028260030990915935 1.7126967332283252\n",
      "Lasso weights range: \n",
      " 0.0 1.8591431326238987\n",
      "ElasticNet weights range: \n",
      " 0.02289057103996131 1.7542913706926486\n"
     ]
    }
   ],
   "source": [
    "print('LinearRegression weights range: \\n', np.abs(lrRegressor.coef_).min(), np.abs(lrRegressor.coef_).max())\n",
    "print('Ridge weights range: \\n', np.abs(ridgeRegressor.coef_).min(), np.abs(ridgeRegressor.coef_).max())\n",
    "print('Lasso weights range: \\n', np.abs(LassoRegressor.coef_).min(), np.abs(LassoRegressor.coef_).max())\n",
    "print('ElasticNet weights range: \\n', np.abs(enRegressor.coef_).min(), np.abs(enRegressor.coef_).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of all regularized models are lowered compared to LinearRegression, with some of the weights of Lasso and ElasticNet shrinked all the way to 0. Using sparsity, the Lasso regularization reduces the number of features, performing feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
