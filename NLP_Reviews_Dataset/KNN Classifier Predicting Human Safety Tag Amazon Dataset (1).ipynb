{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors Model for the Product Safety Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use the K Nearest Neighbors method to build a classifier to predict the predict the human_tag field of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"C:\\Users\\solharsh\\Desktop\\Data Science\\Amazon\\NLP Amazon Final Project Dataset\\training.csv\",encoding='utf-8', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>title</th>\n",
       "      <th>human_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>47490</td>\n",
       "      <td>15808037321</td>\n",
       "      <td>I ordered a sample of the Dietspotlight Burn, ...</td>\n",
       "      <td>6/25/2018 17:51</td>\n",
       "      <td>1</td>\n",
       "      <td>DO NOT BUY!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>16127</td>\n",
       "      <td>16042300811</td>\n",
       "      <td>This coffee tasts terrible as if it got burnt ...</td>\n",
       "      <td>2/8/2018 15:59</td>\n",
       "      <td>2</td>\n",
       "      <td>Coffee not good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>51499</td>\n",
       "      <td>16246716471</td>\n",
       "      <td>I've been buying lightly salted Planters cashe...</td>\n",
       "      <td>3/22/2018 17:53</td>\n",
       "      <td>2</td>\n",
       "      <td>Poor Quality - Burnt, Shriveled Nuts With Blac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>36725</td>\n",
       "      <td>14460351031</td>\n",
       "      <td>This product is great in so many ways. It goes...</td>\n",
       "      <td>12/7/2017 8:49</td>\n",
       "      <td>4</td>\n",
       "      <td>Very lovey product, good sunscreen, but strong...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>49041</td>\n",
       "      <td>15509997211</td>\n",
       "      <td>My skin did not agree with this product, it wo...</td>\n",
       "      <td>3/21/2018 13:51</td>\n",
       "      <td>1</td>\n",
       "      <td>Not for everyone. Reactions can be harsh.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       doc_id                                               text  \\\n",
       "0  47490  15808037321  I ordered a sample of the Dietspotlight Burn, ...   \n",
       "1  16127  16042300811  This coffee tasts terrible as if it got burnt ...   \n",
       "2  51499  16246716471  I've been buying lightly salted Planters cashe...   \n",
       "3  36725  14460351031  This product is great in so many ways. It goes...   \n",
       "4  49041  15509997211  My skin did not agree with this product, it wo...   \n",
       "\n",
       "              date  star_rating  \\\n",
       "0  6/25/2018 17:51            1   \n",
       "1   2/8/2018 15:59            2   \n",
       "2  3/22/2018 17:53            2   \n",
       "3   12/7/2017 8:49            4   \n",
       "4  3/21/2018 13:51            1   \n",
       "\n",
       "                                               title  human_tag  \n",
       "0                                        DO NOT BUY!          0  \n",
       "1                                    Coffee not good          0  \n",
       "2  Poor Quality - Burnt, Shriveled Nuts With Blac...          0  \n",
       "3  Very lovey product, good sunscreen, but strong...          0  \n",
       "4          Not for everyone. Reactions can be harsh.          1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(r\"C:\\Users\\solharsh\\Desktop\\Data Science\\Amazon\\NLP Amazon Final Project Dataset\\public_test_features.csv\",encoding='utf-8',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>62199</td>\n",
       "      <td>15449606311</td>\n",
       "      <td>Quality of material is great, however, the bac...</td>\n",
       "      <td>3/7/2018 19:47</td>\n",
       "      <td>3</td>\n",
       "      <td>great backpack with strange fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>76123</td>\n",
       "      <td>15307152511</td>\n",
       "      <td>The product was okay but wasn't refined campho...</td>\n",
       "      <td>43135.875</td>\n",
       "      <td>2</td>\n",
       "      <td>Not refined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>78742</td>\n",
       "      <td>12762748321</td>\n",
       "      <td>I normally read the reviews before buying some...</td>\n",
       "      <td>42997.37708</td>\n",
       "      <td>1</td>\n",
       "      <td>Doesnt work, wouldnt recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>64010</td>\n",
       "      <td>15936405041</td>\n",
       "      <td>These pads are completely worthless. The light...</td>\n",
       "      <td>43313.25417</td>\n",
       "      <td>1</td>\n",
       "      <td>The lighter colored side of the pads smells li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>17058</td>\n",
       "      <td>13596875291</td>\n",
       "      <td>The saw works great but the blade oiler does n...</td>\n",
       "      <td>12/5/2017 20:17</td>\n",
       "      <td>2</td>\n",
       "      <td>The saw works great but the blade oiler does n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       doc_id                                               text  \\\n",
       "0  62199  15449606311  Quality of material is great, however, the bac...   \n",
       "1  76123  15307152511  The product was okay but wasn't refined campho...   \n",
       "2  78742  12762748321  I normally read the reviews before buying some...   \n",
       "3  64010  15936405041  These pads are completely worthless. The light...   \n",
       "4  17058  13596875291  The saw works great but the blade oiler does n...   \n",
       "\n",
       "              date  star_rating  \\\n",
       "0   3/7/2018 19:47            3   \n",
       "1        43135.875            2   \n",
       "2      42997.37708            1   \n",
       "3      43313.25417            1   \n",
       "4  12/5/2017 20:17            2   \n",
       "\n",
       "                                               title  \n",
       "0                    great backpack with strange fit  \n",
       "1                                        Not refined  \n",
       "2                     Doesnt work, wouldnt recommend  \n",
       "3  The lighter colored side of the pads smells li...  \n",
       "4  The saw works great but the blade oiler does n...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID             0\n",
      "doc_id         0\n",
      "text           6\n",
      "date           0\n",
      "star_rating    0\n",
      "title          1\n",
      "human_tag      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID             0\n",
      "doc_id         0\n",
      "text           2\n",
      "date           0\n",
      "star_rating    0\n",
      "title          1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill-in the missing values for text below. We will just use the placeholder \"Missing\" here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'].fillna(\"Missing\", inplace=True)\n",
    "test_df['text'].fillna(\"Missing\", inplace=True)\n",
    "train_df['title'].fillna(\"Missing\", inplace=True)\n",
    "test_df['title'].fillna(\"Missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to process the text fields. We will remove stop words and apply stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training dataset is: (63134, 7)\n",
      "The shape of the test dataset is: (15784, 6)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the training dataset is:', train_df.shape)\n",
    "print('The shape of the test dataset is:', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df[\"human_tag\"].plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the values for the human tag are False. Meaning, they were identified as safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\solharsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\solharsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the library and functions\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These words are important for our problem. We don't want to remove them.\n",
    "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \n",
    "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \n",
    "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New stop word list\n",
    "stop_words = [word for word in stop if word not in excluding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "print(len(stop_words))\n",
    "print(len(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(texts): \n",
    "    final_text_list=[]\n",
    "    for sent in texts:\n",
    "        filtered_sentence=[]\n",
    "        \n",
    "        sent = sent.lower() # Lowercase \n",
    "        sent = sent.strip() # Remove leading/trailing whitespace\n",
    "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n",
    "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n",
    "        \n",
    "        for w in word_tokenize(sent):\n",
    "            # We are applying some custom filtering here.\n",
    "            # Check if it is not numeric and its length>2 and not in stop words\n",
    "            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words):  \n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "        final_string = \" \".join(filtered_sentence) #final string of cleaned words\n",
    " \n",
    "        final_text_list.append(final_string)\n",
    "    \n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing the training text field\n",
      "Pre-processing the test text field\n"
     ]
    }
   ],
   "source": [
    "print(\"Pre-processing the training text field\")\n",
    "train_df[\"text\"] = process_text(train_df[\"text\"].tolist()) \n",
    "print(\"Pre-processing the test text field\")\n",
    "test_df[\"text\"] = process_text(test_df[\"text\"].tolist()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply min-max scaling to our rating field so that they will be between 0-1. Note that for scalling the validation and test set we are using the same min and max values computed on the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"star_rating\"] = (train_df[\"star_rating\"] - train_df[\"star_rating\"].min())/(train_df[\"star_rating\"].max()-train_df[\"star_rating\"].min())\n",
    "test_df[\"star_rating\"] = (test_df[\"star_rating\"] - test_df[\"star_rating\"].min())/(test_df[\"star_rating\"].max()-test_df[\"star_rating\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "# Input: \"text\", \"star_rating\"\n",
    "# Target: \"human_tag\"\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df[[\"text\", \"star_rating\"]],\n",
    "                                                  train_df[\"human_tag\"].tolist(),\n",
    "                                                  test_size=0.10,\n",
    "                                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our binary vectors for the text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    " \n",
    "# Initialize the binary count vectorizer\n",
    "tfidf_vectorizer = CountVectorizer(binary=True,\n",
    "                                   max_features=50    # Limit the vocabulary size\n",
    "                                  )\n",
    "# Fit and transform\n",
    "X_train_text_vectors = tfidf_vectorizer.fit_transform(X_train[\"text\"].tolist())\n",
    "# Only transform\n",
    "X_val_text_vectors = tfidf_vectorizer.transform(X_val[\"text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'burn': 5, 'even': 10, 'look': 21, 'made': 22, 'not': 26, 'skin': 35, 'time': 41, 'one': 27, 'take': 39, 'need': 25, 'start': 37, 'put': 30, 'use': 44, 'first': 12, 'would': 49, 'feel': 11, 'like': 19, 'get': 13, 'want': 45, 'product': 28, 'burnt': 6, 'realli': 31, 'good': 14, 'work': 48, 'better': 3, 'make': 23, 'thing': 40, 'could': 8, 'smell': 36, 'buy': 7, 'bought': 4, 'heat': 17, 'much': 24, 'back': 1, 'hot': 18, 'also': 0, 'great': 16, 'return': 32, 'seem': 34, 'tri': 42, 'purcha': 29, 'well': 47, 'bad': 2, 'still': 38, 'review': 33, 'day': 9, 'got': 15, 'two': 43, 'way': 46, 'littl': 20}\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the KNN classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for K: 3 is 0.26700251889168763\n",
      "F1 Score for K: 5 is 0.24694903086862888\n",
      "F1 Score for K: 10 is 0.14906832298136644\n",
      "F1 Score for K: 20 is 0.1191806331471136\n",
      "F1 Score for K: 30 is 0.10124164278892073\n"
     ]
    }
   ],
   "source": [
    "# Let' merge our features\n",
    "X_train_features = np.column_stack((X_train_text_vectors.toarray(), \n",
    "                                    X_train[\"star_rating\"].values)\n",
    "                                  )\n",
    "# Let' merge our features\n",
    "X_val_features = np.column_stack((X_val_text_vectors.toarray(), \n",
    "                                    X_val[\"star_rating\"].values)\n",
    "                                  )\n",
    " \n",
    "K_values = [3, 5, 10, 20, 30]\n",
    " \n",
    "for K in K_values:\n",
    "    knnClassifier = KNeighborsClassifier(n_neighbors=K)\n",
    "    knnClassifier.fit(X_train_features, y_train)\n",
    "    val_predictions = knnClassifier.predict(X_val_features)\n",
    "    print(\"F1 Score for K:\", K, \"is\", f1_score(y_val, val_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will pick the best performing model from above. K=3 yields the best result, we will use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get binary features for the test text field\n",
    "X_test_text_vectors = tfidf_vectorizer.transform(test_df[\"text\"].tolist())\n",
    " \n",
    "# Let's merge the fields of interest\n",
    "X_test_features = np.column_stack((X_test_text_vectors.toarray(), \n",
    "                                  test_df[\"star_rating\"].values)\n",
    "                                  )\n",
    " \n",
    "# Fitting the classifier first\n",
    "knnClassifier = KNeighborsClassifier(n_neighbors=3)\n",
    "knnClassifier.fit(X_train_features, y_train)\n",
    " \n",
    "# Predicting on test features\n",
    "test_predictions = knnClassifier.predict(X_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write predictions to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "result_df = pd.DataFrame()\n",
    "result_df[\"ID\"] = test_df[\"ID\"]\n",
    "result_df[\"human_tag\"] = test_predictions\n",
    " \n",
    "result_df.to_csv(r\"C:\\Users\\solharsh\\Desktop\\Data Science\\Amazon\\NLP Amazon Final Project Dataset\\project_day1_result.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very inaccurate... I put it into the oven that was set to 350F and after 20 minutes it still only read &#60;200!!?? So I went with it and increased the oven temperature until it reached the 300's and I completely burned all of the food that I was baking.\n",
      "______________________________\n",
      "Volume is so low it's not worth buying. and when left on to long it heats up and starts to burn where the speakers touch.\n",
      "______________________________\n"
     ]
    }
   ],
   "source": [
    "print(test_df[\"text\"][13])\n",
    "print('______________________________')\n",
    "print(test_df[\"text\"][27])\n",
    "print('______________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was predicted as not safe by our model. That's great! It seems to have predicted correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to say that this light is very bright,but i wish that you would send me a new ine due to half of the light works now  I haven't even had it longer enough to enjoy it. Please help. I can send a photo if you like with the light burning.\n",
      "______________________________\n"
     ]
    }
   ],
   "source": [
    "print(test_df[\"text\"][44])\n",
    "print('______________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is not perfect yet as the above review was marked incorrectly as not safe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test = test_predictions.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0   0\n",
       "1   0\n",
       "2   0\n",
       "3   0\n",
       "4   0\n",
       "5   0\n",
       "6   0\n",
       "7   0\n",
       "8   0\n",
       "9   0\n",
       "10  0\n",
       "11  0\n",
       "12  0\n",
       "13  1\n",
       "14  0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
